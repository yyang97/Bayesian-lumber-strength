{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99771c14",
   "metadata": {},
   "source": [
    "# Damage model \n",
    "The original model is \n",
    "\n",
    "\\begin{equation}\\label{eq: original model}\n",
    "y^*  = y \\cdot I\\{c \\cdot y > l\\} + \\alpha \\cdot y \\cdot I\\{c \\cdot y < l\\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "# Smooth function\n",
    "\n",
    "Note that we have the indicator function in the model we might consider the smooth function:\n",
    "\\begin{equation}\\label{eq: trans function}\n",
    "S(x;s) = \\frac{1}{1 + \\exp(-s \\cdot x)}\n",
    "\\end{equation},\n",
    "where $s$ is the smoothing hyper-parameter to control the smoothness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb933ea",
   "metadata": {},
   "source": [
    "Then the model becomes \n",
    "$$\n",
    "y^*  = y \\cdot \\frac{1}{1 + \\exp(-s\\cdot(cy-l))} + \\alpha \\cdot y \\cdot \\frac{1}{1 + \\exp(-s\\cdot(l-cy))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca7c62",
   "metadata": {},
   "source": [
    "We have $y \\geq y^*$.\n",
    "\n",
    "The lumber have three groups:\n",
    "\n",
    "- Group 1: $y <l$, $y^* < l$, i.e., $y^* <y < l$. The lumber pieces are broken blow the proof loading.\n",
    "- Group 2: $y >l$, $y^* < l$, i.e. $y^*<l<y$. The lumber pieces are broken during the proof loading process. This groups we only knows how many pieces. \n",
    "\n",
    "update: this group should be $0<y^*<l<y$. So $F_y(h^{-1}(l)) - F_y(h^{-1}(\\max(0,h^{-1}(l)))$\n",
    "- Group 3: $y >l$, $y^* > l$, i.e. $l<y^*<y$. The lumber pieces survived in the proof-loading. And then we destruct them to test their strength.\n",
    "\n",
    "\n",
    "# The PDF calculation \n",
    "\n",
    "Given $Y \\sim N(\\mu, \\sigma^2)$, $Y^* = h(Y)$. Then the pdf of $Y^*$,\n",
    "$$\n",
    "f_{Y^*}(y^*) = f_{Y}(h^{-1}(y^*))|\\frac{d}{dy^*}h^{-1}(y^*)|,\n",
    "$$\n",
    "where $f_Y()$ is the pdf of $Y$, i.e., normal. \n",
    "\n",
    "Following this, We need the numerical function of $h^{-1}(y^*)$, and its numerical gradient $\\frac{d}{dy^*}h^{-1}(y^*)$. (The analytical form doesn't seem available.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbcc1a1-7850-4a9b-aca9-1517f6ea7ff4",
   "metadata": {},
   "source": [
    "# The range of alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e450c6-958d-4f16-83e1-2d4122d0a054",
   "metadata": {},
   "source": [
    "For the model \n",
    "\n",
    "\\begin{equation}\n",
    "y^*  = y \\cdot I\\{c \\cdot y > l\\} + \\alpha \\cdot y \\cdot I\\{c \\cdot y < l\\}.\n",
    "\\end{equation}\n",
    "\n",
    "What happened if $\\alpha < c$?\n",
    "\n",
    "then $\\alpha*y < c*y <l $. It means that all damaged pieces are censored. So we don't have damaged pieces. The remaining pieces in group 3 are all undamaged. Then we can only have the range of $\\alpha$ but no specific estimate.\n",
    "\n",
    "In bivarite dataset, we don't have this problem because we have $c*x$ and $\\alpha*y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198c6e6",
   "metadata": {},
   "source": [
    "# Truncated normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44877123",
   "metadata": {},
   "source": [
    "If $\\alpha < c$, it mean that the whole groups are undamaged, which should follow the truncated normal $TN(l/c, \\inf, \\mu,\\sigma)$.\n",
    "\n",
    "If x comes from the truncated normal, we denote $x \\sim TN(a,b,\\mu,\\sigma)$, where the original distribution is $N(\\mu,\\sigma)$ but truncated in the interval $[a,b]$. The package ``scipy.stats.truncnorm`` can calculate the mean and variance of the distribution $TN(a,b,\\mu,\\sigma)$, denoted as $\\mu_T$ and $\\sigma^2_{T}$. \n",
    "\n",
    "Further, if the samples $x_1,...,x_N$ come from $x \\sim TN(a,b,\\mu,\\sigma)$, the sample mean $\\bar x$ asymptotically follows the normal distrbution of mean $mu_T$ and variance $\\sigma^2_T/N$ (by central limit theorem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e2b73",
   "metadata": {},
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0909057",
   "metadata": {},
   "source": [
    "Now according to the model, $\\alpha$ cannot be estimated if $\\alpha\\leq c$;$\\alpha$ can be estimated if $\\alpha>c$; \n",
    "\n",
    "\n",
    "\n",
    "So $H_0:\\alpha\\leq c$, and $H_A: \\alpha>c$. \n",
    "\n",
    "\n",
    "Under $H_0$, the data in the group3, should follow $TN(l/c, \\inf, \\mu,\\sigma)$, of which the mean if $\\mu_T$. Thus we can test whether the mean of group 3, denoted as $\\mu_{g3}$ is  $\\mu_T$ ($\\mu_{g3} = \\mu_T$?).\n",
    "\n",
    "\n",
    "Test statistics:\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\frac{\\mu_{g3} - E[TN(l/\\hat{c},\\inf,\\hat\\mu,\\hat\\sigma)]}{\\sqrt{Var[TN(l/\\hat{c},\\inf,\\hat\\mu,\\hat\\sigma)]/N_{g3}}} \\sim N(0,1)\n",
    "\\end{equation}\n",
    "\n",
    "And p value is $\\Phi(z)$, where $\\Phi$ is the standard normal cdf function. If p-value < 0.05, it means the damage effect is significant. Otherwise, it is insignificant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "51ee4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jaxopt\n",
    "import jax.numpy as jnp\n",
    "import pyreadr\n",
    "import projplot as pjp\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "daed985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "\n",
    "def indicator(x):\n",
    "    return(jnp.select([x>0,x<=0],[1,0]))\n",
    "\n",
    "def logit(x):\n",
    "    return(jnp.log(x/(1-x)))\n",
    "\n",
    "\n",
    "def expit(x):\n",
    "    return 1/(1+jnp.exp(-x))\n",
    "\n",
    "def exp_smooth(x):\n",
    "    return(jnp.select(\n",
    "    [x >0, x<=0],[jnp.exp(-1/(x+ 0.001)),0]))\n",
    "\n",
    "def g_smooth(x):\n",
    "    return(exp_smooth(x)/(exp_smooth(x) + exp_smooth(1-x)))\n",
    "\n",
    "def sigmoid(x, s):\n",
    "    # x = jnp.array(x)\n",
    "    # a = jnp.array(a)\n",
    "    return 0.5 * (jnp.tanh(x * s / 2) + 1)\n",
    "\n",
    "def dmgmodel_ind(y,alpha,l,c):\n",
    "    return(y*indicator(c*y-l) + alpha*y*indicator(l-c*y))\n",
    "\n",
    "\n",
    "def dmgmodel_py(y,alpha,l,c,s):\n",
    "    #return(y*jax.scipy.stats.norm.cdf(c*y-l) + alpha*y*jax.scipy.stats.norm.cdf(l-c*y))\n",
    "    return(y*sigmoid(c*y-l,s) + alpha*y*sigmoid(l-c*y,s))\n",
    "\n",
    "    #return(y*g_smooth(c*y-l) + alpha*y*g_smooth(l-c*y))\n",
    "\n",
    "def dmgmodel_root_py(y,alpha,l,c,s,ystar):\n",
    "    return(dmgmodel_py(y,alpha,l,c,s) - ystar)\n",
    "\n",
    "\n",
    "\n",
    "def dmginverse_py(ystar,alpha,l,c,s):\n",
    "    ystar = jnp.array(ystar)\n",
    "    bisec = jaxopt.Bisection(\n",
    "        optimality_fun=dmgmodel_root_py,\n",
    "        lower = 0,\n",
    "        upper = 10000,\n",
    "        check_bracket = False)\n",
    "    return(bisec.run(alpha = alpha,l = l, c= c ,s = s,ystar = ystar).params)\n",
    "\n",
    "def dmginvgrad_py(ystar,alpha,l,c,s):\n",
    "    grad_func = jax.grad(dmginverse_py,0)\n",
    "    return(jnp.abs(grad_func(ystar,alpha,l,c,s)))\n",
    "\n",
    "def dmglik_py(ystar,alpha,l,c,s,mu,sigma):\n",
    "    y =  dmginverse_py(ystar,alpha,l,c,s)\n",
    "    return(jax.scipy.stats.norm.logpdf(y,loc = mu,scale = sigma)+ \n",
    "           jnp.log(dmginvgrad_py(ystar,alpha,l,c,s))\n",
    "          )\n",
    "\n",
    "\n",
    "def dmglik_vmap(y_group,alpha,l,c,s,mu,sigma):\n",
    "    y_group = jnp.array(y_group)\n",
    "    lik = jax.vmap(lambda y_group: dmglik_py(ystar = y_group,\n",
    "                                             alpha = alpha,l = l, c= c,s =s,mu = mu, sigma=sigma))(y_group)\n",
    "    return(jnp.sum(lik))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093312c-9ad3-4076-ab2a-d5cf36703a28",
   "metadata": {},
   "source": [
    "# In the first case, $\\alpha >c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ffff8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the orignal sample size \n",
    "#N = 30000\n",
    "N = 300\n",
    "mu = 48\n",
    "sigma = 19\n",
    "l =  32\n",
    "\n",
    "c = 0.69\n",
    "s = 10\n",
    "\n",
    "N_marginal = 139\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a49c08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data generation \n",
    "key = jax.random.PRNGKey(0)\n",
    "subkeys = jax.random.split(key, num=1000)\n",
    "\n",
    "repN = 200\n",
    "\n",
    "result = np.empty(repN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b27c77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# negdmglik_jax(theta0)\n",
    "#dmglik_vmap(y_group = y_obs_g3,alpha = alpha,l = l, c = c,s = s, mu = mu, sigma = sigma)\n",
    "\n",
    "#dmglik_py(y_obs_g3[0][0],alpha,l,c,s,mu,sigma)\n",
    "\n",
    "#dmglik_vmap(y_obs_g3,alpha,l,c,s,mu,sigma)\n",
    "\n",
    "for ii in range(repN):\n",
    "\n",
    "    y = sigma*jax.random.normal(subkeys[ii], shape=(N, )) + mu\n",
    "\n",
    "    #y = y[y>0]\n",
    "\n",
    "    # g1\n",
    "    y_obs_g1  = y[y<l]\n",
    "\n",
    "\n",
    "    # g23_star\n",
    "    g23 = y[y>l]\n",
    "    g23_star = jax.vmap(lambda y: dmgmodel_py(y,alpha,l,c,s))(g23)\n",
    "\n",
    "\n",
    "    # g3\n",
    "    y_obs_g3 = g23_star[g23_star > l]\n",
    "\n",
    "    # g2\n",
    "    y_obs_g2 = N - len(y_obs_g1) - len(y_obs_g3)\n",
    "\n",
    "    #y_obs_g2 = jnp.shape(y)[0] - len(y_obs_g1) - len(y_obs_g3)\n",
    "\n",
    "    # g4, is all marginal data\n",
    "    y_obs_g4 = sigma*jax.random.normal(subkeys[0], shape=(N_marginal, )) + mu\n",
    "\n",
    "    s= 10\n",
    "    l = l \n",
    "    y_obs_g1 = y_obs_g1\n",
    "    y_obs_g2 = y_obs_g2\n",
    "    y_obs_g3 = y_obs_g3\n",
    "    y_obs_g4 = y_obs_g4\n",
    "    @jax.jit\n",
    "    def negdmglik_jax(theta):\n",
    "        mu = theta[0]\n",
    "        sigma = theta[1]\n",
    "        alpha = theta[2]\n",
    "        c = theta[3]\n",
    "        lik1 = jnp.sum(jax.scipy.stats.norm.logpdf(y_obs_g1,loc = mu, scale = sigma))\n",
    "        #lik1 = dmglik_vmap(y_group = y_obs_g1,alpha = alpha,l = l, c = c,s = s, mu = mu, sigma = sigma)\n",
    "    #     lik2 = y_obs_g2*jnp.log(\n",
    "    #         jax.scipy.stats.norm.cdf(dmginverse_py(l,alpha,l,c,s), loc=mu, scale=sigma) - \n",
    "    #         jax.scipy.stats.norm.cdf(l, loc=mu, scale=sigma)\n",
    "    #     )\n",
    "        lik2 = y_obs_g2*jnp.log(\n",
    "        jax.scipy.stats.norm.cdf(dmginverse_py(l,alpha,l,c,s), loc=mu, scale=sigma) - \n",
    "        jax.scipy.stats.norm.cdf(dmginverse_py(jnp.maximum(0.1,dmgmodel_py(l,alpha,l,c,s)),alpha,l,c,s), loc=mu, scale=sigma)\n",
    "        )\n",
    "        lik3 = dmglik_vmap(y_group = y_obs_g3,alpha = alpha,l = l, c = c,s = s, mu = mu, sigma = sigma)\n",
    "        lik4 = jnp.sum(jax.scipy.stats.norm.logpdf(y_obs_g4,loc = mu, scale = sigma))\n",
    "\n",
    "        return(-lik1 - lik2-lik3-lik4)\n",
    "\n",
    "    theta0 = jnp.array([mu,sigma,alpha,c])\n",
    "\n",
    "\n",
    "\n",
    "    theta0 = jnp.array([mu,sigma,alpha,c])\n",
    "\n",
    "    solver = jaxopt.ScipyMinimize(method = \"Nelder-Mead\",fun=negdmglik_jax)\n",
    "    res = solver.run(theta0)\n",
    "\n",
    "    # solver = jaxopt.BFGS(fun=negdmglik_jax)\n",
    "    # res = solver.run(theta0)\n",
    "\n",
    "\n",
    "    trunc_mean = scipy.stats.truncnorm.mean((l/res.params[3] -res.params[0])/res.params[1] , \n",
    "                                             float(\"inf\"), loc=res.params[0], scale=res.params[1])\n",
    "    trunc_var = scipy.stats.truncnorm.var((l/res.params[3] -res.params[0])/res.params[1] , \n",
    "                                             float(\"inf\"), loc=res.params[0], scale=res.params[1])\n",
    "\n",
    "    sample_mean = trunc_mean\n",
    "    sample_var = trunc_var/y_obs_g3.shape[-1]\n",
    "\n",
    "\n",
    "    test_stat = (np.mean(y_obs_g3) - sample_mean)/np.sqrt(sample_var)\n",
    "    pval = scipy.stats.norm.cdf(test_stat)\n",
    "    if(ii%50 ==0):\n",
    "        print(ii)\n",
    "    if(pval < 0.05):\n",
    "        result[ii] = 1\n",
    "    else:\n",
    "        result[ii] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8c0e782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if(pval < 0.05):\n",
    "#     res[ii] = 1\n",
    "# else:\n",
    "#     res[ii] = 0\n",
    "#res = res.at[ii].set(1)\n",
    "\n",
    "np.mean(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
